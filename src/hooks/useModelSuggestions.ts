/**
 * @file useModelSuggestions.ts
 * @description Hook para busca de sugestÃµes de modelos
 * @version 1.37.45
 *
 * FASE 47: ExtraÃ­do do App.tsx para consolidar lÃ³gica de sugestÃµes de modelos.
 *
 * Responsabilidades:
 * - Buscar sugestÃµes de modelos para tÃ³picos
 * - Refinar candidatos usando IA
 * - Usar cache para evitar chamadas repetidas
 * - Suportar IA local (embeddings) e API
 *
 * ğŸ”‘ ESTRATÃ‰GIA ZUSTAND: Acessa models via useModelsStore.getState()
 */

import { useCallback } from 'react';
import { useModelsStore } from '../stores/useModelsStore';
import AIModelService from '../services/AIModelService';
import { AI_PROMPTS } from '../prompts';
import { isSpecialTopic } from '../utils/text';
import type { Model, Topic, AIMessage, AICallOptions } from '../types';

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// TIPOS
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

export interface AIIntegrationForSuggestions {
  aiSettings: {
    useLocalAIForSuggestions?: boolean;
    modelSemanticThreshold?: number;
  };
  buildApiRequest: (messages: AIMessage[], optionsOrMaxTokens?: AICallOptions | number) => Record<string, unknown>;
  callAI: (messages: AIMessage[], options?: {
    maxTokens?: number;
    useInstructions?: boolean;
    disableThinking?: boolean;
    logMetrics?: boolean;
    temperature?: number;
    topP?: number;
    topK?: number;
  }) => Promise<string>;
}

export interface APICacheForSuggestions {
  get: (key: string) => unknown | null;
  set: (key: string, value: unknown) => void;
}

export interface UseModelSuggestionsProps {
  /** IntegraÃ§Ã£o com IA */
  aiIntegration: AIIntegrationForSuggestions;
  /** Cache de API */
  apiCache: APICacheForSuggestions;
  /** Se o modelo de busca semÃ¢ntica estÃ¡ pronto */
  searchModelReady: boolean;
}

export interface SuggestionsResult {
  suggestions: Model[];
  source: 'local' | 'api' | null;
}

export interface UseModelSuggestionsReturn {
  /** Busca sugestÃµes de modelos para um tÃ³pico */
  findSuggestions: (topic: Topic) => Promise<SuggestionsResult>;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// FUNÃ‡ÃƒO DE PONTUAÃ‡ÃƒO LOCAL
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Calcula pontuaÃ§Ã£o de relevÃ¢ncia entre um modelo e um tÃ³pico
 */
function scoreModel(model: Model, topicTitle: string, topicCategory: string, topicRelatorio: string): number {
  let score = 0;

  const titleLower = (topicTitle || '').toLowerCase();
  const categoryLower = (topicCategory || '').toLowerCase();
  const relatorioLower = (topicRelatorio || '').toLowerCase();
  const modelTitleLower = (model.title || '').toLowerCase();
  const modelCategoryLower = (model.category || '').toLowerCase();
  const modelKeywordsLower = (typeof model.keywords === 'string' ? model.keywords : (model.keywords || []).join(' ')).toLowerCase();
  const modelContentLower = (model.content || '').toLowerCase();

  // PontuaÃ§Ã£o por tÃ­tulo
  if (modelTitleLower.includes(titleLower) || titleLower.includes(modelTitleLower)) {
    score += 50;
  }

  // PontuaÃ§Ã£o por categoria
  if (categoryLower && modelCategoryLower && categoryLower === modelCategoryLower) {
    score += 30;
  }

  // PontuaÃ§Ã£o por keywords
  const keywords = modelKeywordsLower.split(/[,;\s]+/).filter(k => k.length > 2);
  keywords.forEach(keyword => {
    if (titleLower.includes(keyword) || relatorioLower.includes(keyword)) {
      score += 10;
    }
  });

  // PontuaÃ§Ã£o por conteÃºdo
  const titleWords = titleLower.split(/\s+/).filter(w => w.length > 3);
  titleWords.forEach(word => {
    if (modelContentLower.includes(word)) {
      score += 5;
    }
  });

  return score;
}

// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
// HOOK
// â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

/**
 * Hook para busca de sugestÃµes de modelos
 *
 * @param props - DependÃªncias necessÃ¡rias
 */
export function useModelSuggestions({
  aiIntegration,
  apiCache,
  searchModelReady,
}: UseModelSuggestionsProps): UseModelSuggestionsReturn {
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // REFINAMENTO COM IA
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const refineWithAI = useCallback(async (
    topCandidates: Model[],
    topicTitle: string,
    topicCategory: string,
    topicRelatorio: string
  ): Promise<Model[]> => {
    if (topCandidates.length === 0) return [];

    try {
      const prompt = `${AI_PROMPTS.roles.relevancia}

CONTEXTO DO TÃ“PICO:
TÃ­tulo: ${topicTitle}
Categoria: ${topicCategory || 'NÃ£o especificada'}
Mini-relatÃ³rio: ${topicRelatorio || 'NÃ£o disponÃ­vel'}

MODELOS CANDIDATOS:
${topCandidates.map((m: Model, i: number) => `${i + 1}. [ID: ${m.id}] ${m.title}
   Categoria: ${m.category || 'N/A'}
   Keywords: ${m.keywords || 'N/A'}
   Resumo: ${m.content || 'N/A'}`).join('\n\n')}

TAREFA:
Analise semanticamente qual desses modelos Ã© mais relevante para o tÃ³pico em questÃ£o.
Considere:
1. Similaridade temÃ¡tica entre tÃ­tulo do tÃ³pico e tÃ­tulo do modelo
2. RelevÃ¢ncia das keywords com o contexto do mini-relatÃ³rio
3. Categoria compatÃ­vel
4. Aplicabilidade do conteÃºdo do modelo ao contexto do tÃ³pico

Responda APENAS com um array JSON contendo os IDs dos modelos ordenados por relevÃ¢ncia (do mais relevante ao menos relevante).
Formato: ["id1", "id2", "id3", ...]

Inclua APENAS modelos que sejam realmente relevantes. Se nenhum for relevante, retorne array vazio: []`;

      // Usar Sonnet 4.5 (modelo padrÃ£o) para recomendaÃ§Ã£o de modelos
      const messages: AIMessage[] = [{
        role: 'user',
        content: [{ type: 'text', text: prompt }]
      }];

      let textContent;
      try {
        textContent = await aiIntegration.callAI(messages, {
          maxTokens: 300,
          useInstructions: false,
          disableThinking: true,
          logMetrics: true,
          temperature: 0.0,
          topP: 0.9,
          topK: 40
        });
      } catch {
        return topCandidates; // Retorna candidatos sem reordenar
      }

      // Extrair array JSON da resposta
      const jsonMatch = textContent.match(/\[[\s\S]*?\]/);
      if (!jsonMatch) {
        return topCandidates;
      }

      const rankedIds = JSON.parse(jsonMatch[0]);

      // Reordenar modelos baseado no ranking da IA
      const ranked: Model[] = [];
      rankedIds.forEach((id: string) => {
        const model = topCandidates.find((m: Model) => m.id === id);
        if (model) ranked.push(model);
      });

      // Adicionar modelos que nÃ£o foram ranqueados pela IA (se houver)
      topCandidates.forEach((m: Model) => {
        if (!ranked.find((r: Model) => r.id === m.id)) {
          ranked.push(m);
        }
      });

      return ranked;

    } catch {
      return topCandidates; // Retorna candidatos sem reordenar em caso de erro
    }
  }, [aiIntegration]);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // BUSCA DE SUGESTÃ•ES
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  const findSuggestions = useCallback(async (topic: Topic): Promise<SuggestionsResult> => {
    // ğŸ”‘ ZUSTAND: Acessa models diretamente do store
    const { models } = useModelsStore.getState();

    // v1.29.05: NÃ£o gerar sugestÃµes para tÃ³picos especiais (RELATÃ“RIO e DISPOSITIVO)
    if (isSpecialTopic(topic)) return { suggestions: [], source: null };

    // v1.28.02: IA Local para sugestÃµes (sem API Claude)
    if (aiIntegration.aiSettings.useLocalAIForSuggestions && searchModelReady && models.some(m => m.embedding?.length === 768)) {
      if (!topic?.title || topic.title.length < 3) return { suggestions: [], source: null };
      // v1.32.22: Usar apenas tÃ­tulo para query mais focada
      const topicText = topic.title;
      const cacheKey = `suggestions_local_${topicText}`;
      const cached = apiCache.get(cacheKey);
      if (cached && typeof cached === 'string') {
        try {
          return JSON.parse(cached) as SuggestionsResult;
        } catch { /* ignore parse error */ }
      }
      try {
        await new Promise(r => setTimeout(r, 0)); // Yield para UI nÃ£o congelar
        // v1.32.20: toLowerCase para E5 case-sensitive
        const qEmb = await AIModelService.getEmbedding(topicText.toLowerCase(), 'query');
        const threshold = (aiIntegration.aiSettings.modelSemanticThreshold || 60) / 100;
        const results = models
          .filter(m => m.embedding?.length === 768)
          .map(m => ({ ...m, similarity: AIModelService.cosineSimilarity(qEmb, m.embedding || []) }))
          .filter(m => m.similarity >= threshold)
          .sort((a, b) => b.similarity - a.similarity)
          .slice(0, 5);
        const result: SuggestionsResult = { suggestions: results, source: 'local' };
        apiCache.set(cacheKey, JSON.stringify(result));
        return result;
      } catch { /* fallback para sistema atual */ }
    }

    if (!topic || !topic.title || topic.title.length < 3) return { suggestions: [], source: null };
    if (models.length === 0) return { suggestions: [], source: null };

    const topicTitle = topic.title;
    const topicCategory = topic.category || '';
    const topicRelatorio = topic.relatorio || topic.editedRelatorio || '';

    // Cache key baseado em tÃ­tulo + categoria + relatÃ³rio do tÃ³pico
    const cacheKey = `suggestions_${topicTitle}_${topicCategory}_${topicRelatorio}`;

    // Verificar cache antes de processar
    const cachedResult = apiCache.get(cacheKey);
    if (cachedResult && typeof cachedResult === 'string') {
      try {
        return JSON.parse(cachedResult) as SuggestionsResult;
      } catch { /* ignore parse error */ }
    }

    // PASSO 1: PontuaÃ§Ã£o local
    const scoredModels = models.map(model => ({
      model,
      score: scoreModel(model, topicTitle, topicCategory, topicRelatorio)
    })).filter(item => item.score > 0);

    // Ordenar por score
    scoredModels.sort((a, b) => b.score - a.score);

    // Pegar top 10 candidatos para refinamento com IA
    const topCandidates = scoredModels.slice(0, 10).map(item => item.model);

    if (topCandidates.length === 0) return { suggestions: [], source: null };

    // PASSO 2: Refinamento semÃ¢ntico com IA
    const refinedModels = await refineWithAI(topCandidates, topicTitle, topicCategory, topicRelatorio);

    // Retornar top 5 modelos mais relevantes
    const topSuggestions = refinedModels.slice(0, 5);
    const result: SuggestionsResult = { suggestions: topSuggestions, source: 'api' };

    // Cachear sugestÃµes refinadas
    apiCache.set(cacheKey, JSON.stringify(result));

    return result;
  }, [aiIntegration.aiSettings.useLocalAIForSuggestions, aiIntegration.aiSettings.modelSemanticThreshold, searchModelReady, apiCache, refineWithAI]);

  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
  // RETORNO
  // â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

  return {
    findSuggestions,
  };
}

export default useModelSuggestions;
